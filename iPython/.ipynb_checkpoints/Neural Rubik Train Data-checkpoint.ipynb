{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HJcRbEqLgs_I",
    "outputId": "e7a2b411-bb81-478c-b858-e79bc88ddbc9"
   },
   "outputs": [],
   "source": [
    "!pip uninstall rubikai==0.1.2 -y --quiet\n",
    "!pip install cubeai --no-cache-dir --upgrade --quiet\n",
    "!pip install seaborn --upgrade --quiet\n",
    "\n",
    "import rubikai as rubik\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import google.colab.files\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fvCp8FJlCriL"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_context(\"talk\")  # larger text\n",
    "sns.set_palette(\"dark\")  # brighter colors\n",
    "plt.rcParams.update({'figure.figsize': (10, 8)})  # bigger figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5zh8X98RQUbw"
   },
   "outputs": [],
   "source": [
    "# sets up google drive for the model saving/loading\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "v3GIY3i9j8tX"
   },
   "outputs": [],
   "source": [
    "def refresh_gdrive_token():\n",
    "  global auth\n",
    "  global drive\n",
    "  global gauth\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "swma4dxb_oN-"
   },
   "outputs": [],
   "source": [
    "SAVE_TO_DRIVE = False\n",
    "\n",
    "# gdrive stuff\n",
    "def upload_file_to_drive(local_filename, remote_filename=None):\n",
    "  refresh_gdrive_token()\n",
    "  if remote_filename is None:\n",
    "    remote_filename = 'drive_' + local_filename\n",
    "  uploaded = drive.CreateFile({'title': remote_filename})\n",
    "  uploaded.SetContentFile(local_filename)\n",
    "  uploaded.Upload()\n",
    "  return\n",
    "\n",
    "\n",
    "def download_model_from_drive(remote_filename, local_filename=None):\n",
    "  refresh_gdrive_token()\n",
    "  if local_filename is None:\n",
    "    local_filename = 'local_' + remote_filename\n",
    "  file_list = drive.ListFile(\n",
    "      {'q': \"title = '{}'\".format(remote_filename)}).GetList()\n",
    "  if file_list:\n",
    "    file_list[0].GetContentFile(local_filename)\n",
    "    model = keras.models.load_model(local_filename)\n",
    "    return model\n",
    "  else:\n",
    "    print('file not found in drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gotiaTw8Zwcj"
   },
   "outputs": [],
   "source": [
    "# save/load model functions\n",
    "def save_model(model, filename):\n",
    "  global SAVE_TO_DRIVE\n",
    "  if SAVE_TO_DRIVE is True:\n",
    "    model.save(filename)\n",
    "    upload_file_to_drive(filename, filename)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "  return download_model_from_drive(filename, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPRAYU1IuJwF"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPSskq2p6zqV"
   },
   "source": [
    "Below are functions related to probability vectors. \n",
    "* `create_prob_vector` generates the \"real\" distribution of cube configurations\n",
    "* `convex_combination` returns a convex combination of two vectors\n",
    "* `convex_combination_probabilities` returns a convex combination of the real distribution and the uniform one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pXvei0SWr9rc"
   },
   "outputs": [],
   "source": [
    "# number of states in each distance from goal\n",
    "count_vector = np.array([1, 12, 114, 1068, 10011, 93840, 878880, 8221632,\n",
    "                         76843595, 717789576, 6701836858, 62549615248,\n",
    "                         583570100997, 5442351625028, 50729620202582,\n",
    "                         472495678811004, 4393570406220123, 40648181519827392,\n",
    "                         368071526203620348, 3e18, 14e18, 19e18, 7e18, 24e15,\n",
    "                         150000, 36, 3])\n",
    "# (taken from http://cube20.org/qtm/)\n",
    "\n",
    "\n",
    "def create_prob_vector(lower, upper):\n",
    "  vec = count_vector[lower:upper]\n",
    "  return vec / np.sum(vec)\n",
    "\n",
    "\n",
    "def convex_combination(v , u, alpha):\n",
    "  \"\"\" return covex combination of u, v i.e v*alpha + u*(1- alpha) \"\"\"\n",
    "  assert len(u) == len(v), 'u ,v must have same length'\n",
    "  assert 0 <= alpha <= 1, 'alpha must be between 0 and 1'\n",
    "  return np.array(v)*alpha + np.array(u)*(1-alpha)\n",
    "\n",
    "def convex_combination_probabilities(alpha, lower, upper):\n",
    "  \"\"\" \n",
    "  returns convex combination of real probability and uniform distribution\n",
    "  real_probabilites*alpha + uniform*(1- alpha)\n",
    "  \"\"\"\n",
    "  rel_prob = create_prob_vector(lower, upper)\n",
    "  n = len(rel_prob)\n",
    "  uniform = np.ones(n) / n\n",
    "  return convex_combination(rel_prob, uniform, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQCZ4MTz_cFe"
   },
   "source": [
    "### Functions for Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "u0fLmKLPtUbQ"
   },
   "outputs": [],
   "source": [
    "def get_features_from_cube(cube):\n",
    "  \"\"\" transforms the cube's array to 1d binary array \"\"\"\n",
    "  binary_array = keras.utils.to_categorical(cube.to_array(), rubik.NUM_FACES)\n",
    "  return binary_array.flatten()\n",
    "\n",
    " \n",
    "def data_generator(cube_layers, max_d, batch_size, p=None):\n",
    "  \"\"\"\n",
    "  generates batches of scrambled cubes data, coupled with the number\n",
    "  of scramble moves per row\n",
    "  \"\"\"\n",
    "  new_dim = len(get_features_from_cube(rubik.Cube(cube_layers)))\n",
    "  while True:\n",
    "    data = np.empty((batch_size, new_dim), dtype=np.int8)\n",
    "    labels = np.empty(batch_size, dtype=np.int8)\n",
    "    for i in range(batch_size):\n",
    "      c = rubik.Cube(cube_layers)\n",
    "      d = np.random.choice(np.arange(max_d+1), p=p)\n",
    "      rand_seq = rubik.generate_random_sequence(cube_layers, d)\n",
    "      c.apply(rand_seq)\n",
    "      data[i, :] = get_features_from_cube(c)\n",
    "      labels[i] = d\n",
    "    yield data, labels  \n",
    "\n",
    "\n",
    "def create_dnnregressor(cube_layers, hidden_units, dropout=None,\n",
    "                        optimizer='adagrad', loss='mse'):\n",
    "  \"\"\"\n",
    "  creates a fully connected multi-layer perceptron with non-linear activations\n",
    "  to perform regression.\n",
    "  \n",
    "  :param cube_layers: the number of cube layers this model should operate on\n",
    "  :param hidden_units: list of integers specifying how many hidden neurons\n",
    "                       are in each layer\n",
    "  :param dropout: if None, no dropout is used. if a single integer, uses this\n",
    "                  dropout rate after each layer. otherwise, should be a list of\n",
    "                  integers the same length as hidden_units specifying dropout \n",
    "                  rate after each layer\n",
    "  :param optimizer: which (keras) optimizer to use\n",
    "  :param loss: which (keras) loss to use\n",
    "  :returns: a compiled keras.Sequential model\n",
    "  \"\"\"\n",
    "  # input checks\n",
    "  assert hasattr(hidden_units, '__len__'), 'hidden_units must be array-like'\n",
    "  assert len(hidden_units) > 0, 'hidden_units cannot be empty'\n",
    "  if dropout is not None:\n",
    "    if not hasattr(dropout, '__len__'):\n",
    "      dropout = [dropout] * len(hidden_units)\n",
    "    else:\n",
    "      assert len(hidden_units) == len(dropout)\n",
    "  # define some constant model parameters\n",
    "  activation = 'relu'\n",
    "  out_activation = 'relu'\n",
    "  input_dim = len(get_features_from_cube(rubik.Cube(cube_layers)))\n",
    "  \n",
    "  # create a sequential model and add the first layer\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Dense(hidden_units[0],\n",
    "                               input_dim=input_dim,\n",
    "                               activation=activation))\n",
    "  if dropout is not None:\n",
    "    model.add(keras.layers.Dropout(dropout[0]))\n",
    "    \n",
    "  # add the rest of the hidden layers\n",
    "  for i in range(1, len(hidden_units)):\n",
    "    model.add(keras.layers.Dense(hidden_units[i], activation=activation))\n",
    "    if dropout is not None:\n",
    "      model.add(keras.layers.Dropout(dropout[i]))\n",
    "\n",
    "  # define the output layer\n",
    "  model.add(keras.layers.Dense(1, activation=out_activation))\n",
    "  model.compile(optimizer=optimizer, loss=loss)\n",
    "  return model\n",
    "\n",
    "\n",
    "def learn_heuristic(layers, max_d, p, steps, epochs, batch_size, hidden_units,\n",
    "                    dropout=None, optimizer='adagrad', loss='mse'):\n",
    "  \"\"\"\n",
    "  trains a model with the given config.\n",
    "  \n",
    "  :param layers: number of cube layers\n",
    "  :param max_d: maximum number of scramble steps\n",
    "  :param p: a probability distribution according to which the\n",
    "            scramble steps number is chosen (array of length max_d+1)\n",
    "            (for uniform dist. use None)\n",
    "  :param steps: number of training steps per epoch\n",
    "  :param epochs: number of epochs\n",
    "  :param batch_size: number of cube instances in each training step\n",
    "  :param hidden_units: number of dnn layers and neurons in each layer\n",
    "                       (an array of integers)\n",
    "  :param dropout: same as in create_dnnregressor\n",
    "  :param optimizer: same as in create_dnnregressor\n",
    "  :param loss: same as in create_dnnregressor\n",
    "  :returns: a pair (estimator, history), where estimator is a (trained)\n",
    "            keras model, and history is the training Keras history object\n",
    "  \"\"\"\n",
    "  # set up parameters\n",
    "  c = rubik.Cube(layers)\n",
    "  # initialize the model\n",
    "  estimator = create_dnnregressor(\n",
    "      cube_layers=layers,\n",
    "      hidden_units=hidden_units,\n",
    "      dropout=dropout,\n",
    "      optimizer=optimizer,\n",
    "      loss=loss\n",
    "  )\n",
    "  # train the model\n",
    "  history = estimator.fit_generator(\n",
    "      data_generator(layers, max_d, batch_size, p), steps, epochs\n",
    "  )\n",
    "  return estimator, history\n",
    "\n",
    "\n",
    "def model_to_heuristic(model):\n",
    "  \"\"\" creates a heuristic based on the given keras model \"\"\"\n",
    "  \n",
    "  def _model_h(cube, problem=None):\n",
    "    features = get_features_from_cube(cube)\n",
    "    return model.predict(np.reshape(features, (1, -1)))[0][0]\n",
    "  \n",
    "  return _model_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oi5RHq-eyWcm"
   },
   "source": [
    "## $3\\times3\\times3$\n",
    "We try different network architectures and see which one performs best.\n",
    "\n",
    "The heuristics are then save in the following format:\n",
    "\n",
    "`<layers>_<max_d>_<hidden_1>_..._<hidden_k>.h5`\n",
    "\n",
    "where `hidden_i` is the number of neurons in the `i`'th hidden layer, and `layers` is the number of layers in the cube, and `max_d` is the maximal number of scramble moves.\n",
    "\n",
    "For example, a $3\\times 3 \\times 3$ model with 3 layers of 50 neurons each, and `max_d=8` is saved as: \n",
    "\"`3_8_50_50_50.h5`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cE3Kl1ODI3KU"
   },
   "outputs": [],
   "source": [
    "def get_model_filename(layers, max_d, hidden_units):\n",
    "  delim = '_'\n",
    "  suffix = '.h5'\n",
    "  return delim.join([str(layers), str(max_d)] + \n",
    "                    [str(h) for h in hidden_units]) + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HV_K9kv1AYkG"
   },
   "source": [
    "### $\\hat h_1$\n",
    "* `max_d` (maximal number of scramble moves):  `10`\n",
    "* `p` (distribution for the number of moves): $0.1 \\cdot \\vec \\rho + 0.9 \\cdot \\vec u$, where $\\vec \\rho$ is the real distribution of cube configurations, and $\\vec u$ is the unfirom distribution\n",
    "* `steps` (training steps per epoch): `100`\n",
    "* `epochs` (number of epochs): `100`\n",
    "* `batch_size` (number of examples per training step): `8`\n",
    "* Net architecture: \n",
    "  *  3 hidden layers with 70, 60, 50 neurons\n",
    "  * default ReLU activations\n",
    "  * No edge dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q-u9iYE7FDz9"
   },
   "outputs": [],
   "source": [
    "layers = 3\n",
    "max_d = 10\n",
    "p = convex_combination_probabilities(0.1, 0, max_d+1)\n",
    "steps = 100\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "dropout = None\n",
    "hidden_units_1 = [70, 60, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "ec0kHJF3Aif2",
    "outputId": "b6862394-d606-4e01-cc18-7387e3d823f0"
   },
   "outputs": [],
   "source": [
    "m1, history1 = learn_heuristic(layers, max_d, p, steps, epochs,\n",
    "                               batch_size, hidden_units_1, dropout)\n",
    "save_model(m1, get_model_filename(layers, max_d, hidden_units_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Pi4N14Z91SE"
   },
   "source": [
    "### $\\hat h_2$\n",
    "* `max_d`: `10`\n",
    "* `p`: $0.1 \\cdot \\vec \\rho + 0.9 \\cdot \\vec u$ (same as in $\\hat h_1$)\n",
    "* `steps`: `100`\n",
    "* `epochs`: `100`\n",
    "* `batch_size`: `8`\n",
    "* Net architecture: \n",
    "  *  4 hidden layers, 50 neurons per layer\n",
    "  * default ReLU activations\n",
    "  * No edge dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "Pja05ir-GAhI",
    "outputId": "5b2e4d1a-0abb-4824-a13c-359b14ff7102"
   },
   "outputs": [],
   "source": [
    "hidden_units_2 = [50, 50, 50, 50]\n",
    "m2, history2 = learn_heuristic(layers, max_d, p, steps, epochs,\n",
    "                               batch_size, hidden_units_2, dropout)\n",
    "save_model(m2, get_model_filename(layers, max_d, hidden_units_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2AyPlrIBXOI"
   },
   "source": [
    "### $\\hat h_3$\n",
    "* `max_d`: `10`\n",
    "* `p`: $0.1 \\cdot \\vec \\rho + 0.9 \\cdot \\vec u$ (same as in $\\hat h_1$)\n",
    "* `steps`: `100`\n",
    "* `epochs`: `100`\n",
    "* `batch_size`: `8`\n",
    "* Net architecture: \n",
    "  *  5 hidden layers with 50, 40, 30, 20, 20 neurons\n",
    "  * default ReLU activations\n",
    "  * No edge dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "E4-bTmMcea2Z",
    "outputId": "f679011c-b928-403b-d94e-473bf20e6001"
   },
   "outputs": [],
   "source": [
    "hidden_units_3 = [50, 40, 30, 20, 20]\n",
    "m3, history3 = learn_heuristic(layers, max_d, p, steps, epochs,\n",
    "                               batch_size, hidden_units_3, dropout)\n",
    "save_model(m3, get_model_filename(layers, max_d, hidden_units_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "mTc1U3ZDdk7V",
    "outputId": "c0da91eb-9994-478b-cdd8-73769c548d6b"
   },
   "outputs": [],
   "source": [
    "histories = pd.DataFrame(data={\n",
    "    '$\\hat h_1$': history1.history['loss'],\n",
    "    '$\\hat h_2$': history2.history['loss'],\n",
    "    '$\\hat h_3$': history3.history['loss'],\n",
    "    'epoch': np.arange(1, epochs+1)\n",
    "}).set_index('epoch')\n",
    "sns.lineplot(data=histories);\n",
    "plt.ylabel('average loss');\n",
    "plt.title('Learning Curves');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XOWlFiniH3pn"
   },
   "outputs": [],
   "source": [
    "histories.to_csv('3_10_training_loss_epoch_1-100.csv')\n",
    "from google.colab import files\n",
    "files.download('3_10_training_loss_epoch_1-100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qnzL4sES5Mn"
   },
   "source": [
    "## Continue Training\n",
    "This section loads already trained models and continues to train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KKQAIKQSFtEq"
   },
   "outputs": [],
   "source": [
    "layers = 3\n",
    "max_d = 10\n",
    "p = convex_combination_probabilities(0.1, 0, max_d+1)\n",
    "steps = 50\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "hidden_units_1 = [70, 60, 50]\n",
    "hidden_units_2 = [50, 50, 50, 50]\n",
    "hidden_units_3 = [50, 40, 30, 20, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N1I8PpFzeOsz"
   },
   "outputs": [],
   "source": [
    "fn1 = get_model_filename(layers, max_d, hidden_units_1)\n",
    "fn2 = get_model_filename(layers, max_d, hidden_units_2)\n",
    "fn3 = get_model_filename(layers, max_d, hidden_units_3)\n",
    "\n",
    "m1 = load_model(fn1)\n",
    "m2 = load_model(fn2)\n",
    "m3 = load_model(fn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10738
    },
    "colab_type": "code",
    "id": "5f80IBjJa3De",
    "outputId": "8424237c-aca6-4c90-9140-15ad6c0abf1c"
   },
   "outputs": [],
   "source": [
    "# add some more training for each of the model\n",
    "history1 = m1.fit_generator(\n",
    "    data_generator(layers, max_d, batch_size, p), steps, epochs)\n",
    "save_model(m1, fn1)\n",
    "history2 = m2.fit_generator(\n",
    "    data_generator(layers, max_d, batch_size, p), steps, epochs)\n",
    "save_model(m2, fn2)\n",
    "history3 = m3.fit_generator(\n",
    "    data_generator(layers, max_d, batch_size, p), steps, epochs)\n",
    "save_model(m3, fn3)\n",
    "\n",
    "histories = pd.DataFrame(data={\n",
    "    '$\\hat h_1$': history1.history['loss'],\n",
    "    '$\\hat h_2$': history2.history['loss'],\n",
    "    '$\\hat h_3$': history3.history['loss'],\n",
    "    'epoch': np.arange(101, 101 + epochs)\n",
    "}).set_index('epoch')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cubeai_model_keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
